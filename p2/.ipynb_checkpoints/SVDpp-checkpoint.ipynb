{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "8e65e39c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "501\n",
      "1001\n",
      "Epoch: 1, Train MAE: 0.9037, Val MAE: 0.8700\n",
      "Epoch: 2, Train MAE: 0.8272, Val MAE: 0.8402\n",
      "Epoch: 3, Train MAE: 0.7924, Val MAE: 0.8257\n",
      "Epoch: 4, Train MAE: 0.7698, Val MAE: 0.8169\n",
      "Epoch: 5, Train MAE: 0.7530, Val MAE: 0.8109\n",
      "Epoch: 6, Train MAE: 0.7396, Val MAE: 0.8063\n",
      "Epoch: 7, Train MAE: 0.7283, Val MAE: 0.8027\n",
      "Epoch: 8, Train MAE: 0.7185, Val MAE: 0.7997\n",
      "Epoch: 9, Train MAE: 0.7096, Val MAE: 0.7972\n",
      "Epoch: 10, Train MAE: 0.7014, Val MAE: 0.7951\n",
      "Epoch: 11, Train MAE: 0.6936, Val MAE: 0.7933\n",
      "Epoch: 12, Train MAE: 0.6861, Val MAE: 0.7919\n",
      "Epoch: 13, Train MAE: 0.6788, Val MAE: 0.7908\n",
      "Epoch: 14, Train MAE: 0.6715, Val MAE: 0.7897\n",
      "Epoch: 15, Train MAE: 0.6643, Val MAE: 0.7888\n",
      "Epoch: 16, Train MAE: 0.6571, Val MAE: 0.7880\n",
      "Epoch: 17, Train MAE: 0.6499, Val MAE: 0.7872\n",
      "Epoch: 18, Train MAE: 0.6426, Val MAE: 0.7865\n",
      "Epoch: 19, Train MAE: 0.6352, Val MAE: 0.7859\n",
      "Epoch: 20, Train MAE: 0.6278, Val MAE: 0.7854\n",
      "Epoch: 21, Train MAE: 0.6203, Val MAE: 0.7849\n",
      "Epoch: 22, Train MAE: 0.6128, Val MAE: 0.7845\n",
      "Epoch: 23, Train MAE: 0.6052, Val MAE: 0.7842\n",
      "Epoch: 24, Train MAE: 0.5976, Val MAE: 0.7839\n",
      "Epoch: 25, Train MAE: 0.5901, Val MAE: 0.7837\n",
      "Epoch: 26, Train MAE: 0.5825, Val MAE: 0.7836\n",
      "Epoch: 27, Train MAE: 0.5751, Val MAE: 0.7835\n",
      "Epoch: 28, Train MAE: 0.5677, Val MAE: 0.7834\n",
      "Epoch: 29, Train MAE: 0.5604, Val MAE: 0.7834\n",
      "Epoch: 30, Train MAE: 0.5533, Val MAE: 0.7834\n",
      "Early stopping after 30 epochs\n",
      "4.31420641010424\n"
     ]
    }
   ],
   "source": [
    "# Title:  CSEN272 Project 2 \n",
    "# Author: Yanxu Wu (W1650780)\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Change test files for different results\n",
    "train_file = 'train.txt'\n",
    "test_file = 'test20.txt'\n",
    "\n",
    "# Load the training data\n",
    "train_data = pd.read_csv(train_file, sep=' ', header=None, names=['user', 'movie', 'rating'])\n",
    "\n",
    "# Load the test data\n",
    "test_data = pd.read_csv(test_file, sep=' ', header=None, names=['user', 'movie', 'rating'])\n",
    "\n",
    "# Split the training data into training and validation sets\n",
    "train_set, val_set = train_test_split(train_data, test_size=0.2, random_state=42)\n",
    "\n",
    "# Get the number of users and movies dimensions\n",
    "n_users = max(train_data['user'].max(), test_data['user'].max()) + 1\n",
    "n_items = max(train_data['movie'].max(), test_data['movie'].max()) + 1\n",
    "print(n_users)\n",
    "print(n_items)\n",
    "\n",
    "# SVD++ Model\n",
    "class SVDpp:\n",
    "    def __init__(self, n_epochs, n_users, n_items, n_factors, lr, reg_rate, random_seed=0):\n",
    "        self.n_epochs = n_epochs\n",
    "        self.lr = lr\n",
    "        self.reg_rate = reg_rate\n",
    "        self.n_factors = n_factors\n",
    "        np.random.seed(random_seed)\n",
    "        self.pu = np.random.randn(n_users, n_factors) / np.sqrt(n_factors)\n",
    "        self.qi = np.random.randn(n_items, n_factors) / np.sqrt(n_factors)\n",
    "        self.yj = np.random.randn(n_items, n_factors) / np.sqrt(n_factors)\n",
    "        self.bu = np.zeros(n_users, np.double)\n",
    "        self.bi = np.zeros(n_items, np.double)\n",
    "        self.global_bias = 0\n",
    "        self.Iu = {u: [] for u in range(n_users)}\n",
    "        \n",
    "    def reg_sum_yj(self, u):\n",
    "        sum_yj = np.zeros(self.n_factors, np.double)\n",
    "        for j in self.Iu[u]:\n",
    "            sum_yj += self.yj[j]\n",
    "        return sum_yj / np.sqrt(len(self.Iu[u])) if self.Iu[u] else sum_yj\n",
    "        \n",
    "    def predict(self, u, i, feedback_vec_reg):\n",
    "        return self.global_bias + self.bu[u] + self.bi[i] + np.dot(self.qi[i], self.pu[u] + feedback_vec_reg)\n",
    "        \n",
    "    def fit(self, train_set, val_set, verbose=True, patience=2, min_delta=0.0001):\n",
    "        self.global_bias = np.mean(train_set.rating)\n",
    "        # Record the items rated by each user\n",
    "        g = train_set.groupby(['user'])\n",
    "        for uid, df_uid in g:\n",
    "            self.Iu[uid] = list(df_uid['movie'])\n",
    "        \n",
    "        best_val_mae = float('inf')\n",
    "        epochs_no_improve = 0\n",
    "        \n",
    "        for epoch in range(self.n_epochs):\n",
    "            total_error = 0\n",
    "            for index, row in train_set.iterrows():\n",
    "                u, i, r = int(row['user']), int(row['movie']), row['rating']\n",
    "                feedback_vec_reg = self.reg_sum_yj(u)\n",
    "                error = r - self.predict(u, i, feedback_vec_reg)\n",
    "                total_error += abs(error)\n",
    "                self.bu[u] += self.lr * (error - self.reg_rate * self.bu[u])\n",
    "                self.bi[i] += self.lr * (error - self.reg_rate * self.bi[i])\n",
    "                tmp_pu = self.pu[u]\n",
    "                tmp_qi = self.qi[i]\n",
    "                self.pu[u] += self.lr * (error * self.qi[i] - self.reg_rate * self.pu[u])\n",
    "                self.qi[i] += self.lr * (error * (tmp_pu + feedback_vec_reg) - self.reg_rate * self.qi[i])\n",
    "                for j in self.Iu[u]:\n",
    "                    self.yj[j] += self.lr * (error / np.sqrt(len(self.Iu[u])) * tmp_qi - self.reg_rate * self.yj[j])\n",
    "            \n",
    "            train_mae = total_error / len(train_set)\n",
    "            val_mae = self.evaluate(val_set)\n",
    "            \n",
    "            if verbose:\n",
    "                print(f'Epoch: {epoch+1}, Train MAE: {train_mae:.4f}, Val MAE: {val_mae:.4f}')\n",
    "            \n",
    "            # Check for improvement\n",
    "            if best_val_mae - val_mae > min_delta:\n",
    "                best_val_mae = val_mae\n",
    "                epochs_no_improve = 0\n",
    "            else:\n",
    "                epochs_no_improve += 1\n",
    "                \n",
    "            # Early stopping\n",
    "            if epochs_no_improve >= patience:\n",
    "                print(f'Early stopping after {epoch+1} epochs')\n",
    "                break\n",
    "                \n",
    "        return self\n",
    "    \n",
    "    def evaluate(self, test_set):\n",
    "        predictions = test_set.apply(lambda x: self.predict(int(x['user']), int(x['movie']), self.reg_sum_yj(int(x['user']))), axis=1)\n",
    "        mae = np.mean(np.abs(test_set['rating'] - predictions))\n",
    "        return mae\n",
    "    \n",
    "    def predict_single(self, user, movie):\n",
    "        return self.predict(user, movie, self.reg_sum_yj(user))\n",
    "\n",
    "# Train the model on the training data\n",
    "svdpp = SVDpp(n_epochs=50, n_users=n_users, n_items=n_items, n_factors=30, lr=0.008, reg_rate=0.09)\n",
    "svdpp.fit(train_set, val_set, verbose=True, patience=2, min_delta=0.0001)\n",
    "\n",
    "# Predict a single rating from train data for testing purpose\n",
    "predicted_single_rating = svdpp.predict_single(117, 1)\n",
    "print(predicted_single_rating)\n",
    "\n",
    "# Prepare to generate predictions for unknown ratings\n",
    "result = []\n",
    "\n",
    "# Iterate over each user block in the test data\n",
    "for user in range(401, 501):  # Adjust user range based on the test file (401, 501 for test20.txt)\n",
    "    user_test_data = test_data[test_data['user'] == user]\n",
    "    unknown_ratings = user_test_data[user_test_data['rating'] == 0]\n",
    "\n",
    "    if len(unknown_ratings) > 0:\n",
    "        # Predict the unknown ratings\n",
    "        for index, row in unknown_ratings.iterrows():\n",
    "            u, m = int(row['user']), int(row['movie'])\n",
    "            predicted_rating = int(round(svdpp.predict_single(u, m)))\n",
    "            predicted_rating = min(max(predicted_rating, 1), 5)\n",
    "            result.append((u, m, predicted_rating))\n",
    "\n",
    "# Convert the result list to a DataFrame and sort it\n",
    "result_df = pd.DataFrame(result, columns=['user', 'movie', 'rating'])\n",
    "result_df.sort_values(by=['user', 'movie'], inplace=True)\n",
    "\n",
    "# Save the predictions (change to result20.txt for test20.txt)\n",
    "result_df.to_csv('result20.txt', sep=' ', header=False, index=False)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05cb43b3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
